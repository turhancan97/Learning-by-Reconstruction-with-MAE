seed: 12345
logging:
  wandb_log: False
  wandb_project: "Learning_by_Reconstruction"
  wandb_run_name: "MAE"
PCA:
  dataset: cifar10 # imagenet, cifar10, cifar100, stl10 or custom
  split: val # train, val or test
  use_sklearn: False
  resize: 256
  crop: 224
  variance_cutoff: 0.25
MAE:
  dataset: cifar10 # imagenet, cifar10, cifar100, stl10 or custom
  pca_mode: no_mode # bottom_20, bottom_25, bottom_30, bottom_35, bottom_40
  batch_size: 4096
  max_device_batch_size: 512
  base_learning_rate: 1.5e-4
  weight_decay: 0.05
  mask_ratio: 0.75
  total_epoch: 400
  warmup_epoch: 40
  model_name: vit-t-mae-pretrain.pt
  MODEL:
    image_size: 32
    patch_size: 2
    emb_dim: 192
    encoder_layer: 12
    encoder_head: 3
    decoder_layer: 4
    decoder_head: 3
FINETUNE:
  batch_size: 128
  max_device_batch_size: 256
  base_learning_rate: 1.0e-3
  weight_decay: 0.05
  label_smoothing: 0.1
  total_epoch: 100
  warmup_epoch: 5
  output_model_path: vit-t-mae-finetune.pt
LINPROBE:
  batch_size: 256
  max_device_batch_size: 256
  base_learning_rate: 0.1
  weight_decay: 0
  total_epoch: 90
  warmup_epoch: 10
  output_model_path: vit-t-mae-linprobe.pt